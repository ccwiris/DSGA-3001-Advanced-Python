{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 13: Pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resilient distributed dataset (RDD)\n",
    "\n",
    "- Dataset contains a collection of elements of any type\n",
    "- Dataset can be partitioned and distributed across multiple nodes\n",
    "- RDDs are immutable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lazy evaluation\n",
    "\n",
    "Spark will not load or transform data unless an action is performed\n",
    "\n",
    "- Load file into RDD\n",
    "- filter the RDD\n",
    "- count the number of elements (load and filter happen now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "import random\n",
    "sc = SparkContext(master=\"local\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sc.textFile('auto-data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns the RDD as a list\n",
    "data.collect() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.count()  #Number of lines in rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.first()) #Prints first line\n",
    "print()\n",
    "print(data.take(5)) #Prints first 5 lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations\n",
    "\n",
    "- Perform operations on an RDD and create a new RDD\n",
    "- Lazy evaluation\n",
    "- Can be distributed across multiple nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map\n",
    "\n",
    "newRDD = rdd.map(function)\n",
    "\n",
    "- Result can be a different type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Map and create a new RDD\n",
    "tsvdata = data.map(lambda x: x.replace(',','\\t'))\n",
    "tsvdata.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter\n",
    "\n",
    "newRDD = rdd.filter(function)\n",
    "\n",
    "- Filter an RDD to select elements that match a condition\n",
    "- Result RDD smaller than original RDD\n",
    "- Function should return true/false for each element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter and create a new RDD\n",
    "toyotadata = data.filter(lambda x: 'toyota' in x)\n",
    "print(toyotadata.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set operations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words1 = sc.parallelize(['knees','weak','arms','heavy'])\n",
    "words2 = sc.parallelize(['new','words','arms','knees'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(words1.union(words2).distinct().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(words1.intersection(words2).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce\n",
    "\n",
    "- Perform an operation across all elements of an RDD (sum, count, etc)\n",
    "- Operation is a function with two inputs\n",
    "- Function is called for every element in the RDD\n",
    "\n",
    "rdd = [a,b,c,d,e] and function is f(x,y)\n",
    "\n",
    "   - f(f(f(f(a,b),c),d),e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata = sc.parallelize(range(10))\n",
    "print(newdata.reduce(lambda x,y : x+y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shortest line in RDD\n",
    "line = data.reduce(lambda x,y: x if len(x) < len(y) else y)\n",
    "print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Preprocessing\n",
    "def preprocess(line):\n",
    "    lis = line.split(',')\n",
    "    \n",
    "    #Convert doors to integer\n",
    "    if lis[3] == 'two':\n",
    "        lis[3] = '2'\n",
    "    else:\n",
    "        lis[3] = '4'\n",
    "    \n",
    "    #Convert drive to uppercase\n",
    "    lis[5] = lis[5].upper()\n",
    "    return ','.join(lis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_data = data.map(preprocess)\n",
    "preprocessed_data.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute average miles per gallon (9th column)\n",
    "def mpgavg(line):\n",
    "    if isinstance(line,int):\n",
    "        return line\n",
    "    lis = line.split(',')\n",
    "    \n",
    "    if lis[9].isdigit():\n",
    "        return int(lis[9])\n",
    "    \n",
    "    return 0 #Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.reduce(lambda x,y: mpgavg(x) + mpgavg(y)) / (data.count()-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shared variables\n",
    "\n",
    "- __Broadcast variables__ are distributed to all workers, but are read-only. These variables can be used as lookup tables or stopword lists.\n",
    "- __Accumulators__ are variables that workers can “add” to using associative operations and are typically used as counters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize accumulator\n",
    "sedans = sc.accumulator(0)\n",
    "hatchbacks = sc.accumulator(0)\n",
    "\n",
    "#Initialize broadcast variables\n",
    "stext = sc.broadcast('sedan')\n",
    "htext = sc.broadcast('hatchback')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_lines(line):\n",
    "    \n",
    "    global sedans\n",
    "    global hatchbacks\n",
    "    \n",
    "    if stext.value in line:\n",
    "        sedans += 1\n",
    "    if htext.value in line:\n",
    "        hatchbacks += 1\n",
    "    \n",
    "    return line.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_data = data.map(split_lines).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sedans, hatchbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SparkSQL\n",
    "\n",
    "- Library that supports sql like data and operations\n",
    "- Dataframe - Collections of data opganized as rows and columns\n",
    "\n",
    "Operations supported by dataframes\n",
    "\n",
    "- filter - filter data based on a condition\n",
    "- join - join two dataframes based on a column value\n",
    "- groupby - group data grames by specific column values\n",
    "\n",
    "etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlcontext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emdf = sqlcontext.read.json('customerData.json')\n",
    "emdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SQL queries\n",
    "\n",
    "emdf.select('name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emdf.filter(emdf['age'] == 40).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emdf.groupBy('gender').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
